{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Step 1: Pseudo-Labeling Generator\n",
                "\n",
                "This notebook uses the initial 15-model ensemble to label high-confidence test samples and expand the training set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, json, numpy as np, pandas as pd, torch, torch.nn as nn, torch.nn.functional as F, torchaudio, torchvision.models as models\n",
                "from tqdm.auto import tqdm\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "MODEL_PATH = '/kaggle/input/the-last-frequency-models'\n",
                "DATA_DIR = '/kaggle/input/the-last-frequency'\n",
                "THRESHOLD = 0.98  # Only take samples where model is extremely confident\n",
                "\n",
                "class CFG:\n",
                "    sample_rate, n_fft, hop_length, n_mels, target_frames = 16000, 1024, 256, 128, 64\n",
                "    batch_size, num_classes = 128, 35\n",
                "\n",
                "class SpecTransform(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.mel_spec = torchaudio.transforms.MelSpectrogram(sample_rate=CFG.sample_rate, n_fft=CFG.n_fft, hop_length=CFG.hop_length, n_mels=CFG.n_mels)\n",
                "        self.amp_to_db = torchaudio.transforms.AmplitudeToDB()\n",
                "    def forward(self, x):\n",
                "        x = self.amp_to_db(self.mel_spec(x))\n",
                "        if x.shape[-1] > CFG.target_frames: x = x[..., :CFG.target_frames]\n",
                "        elif x.shape[-1] < CFG.target_frames: x = F.pad(x, (0, CFG.target_frames - x.shape[-1]))\n",
                "        return x.unsqueeze(1)\n",
                "\n",
                "class AudioResNet(nn.Module):\n",
                "    def __init__(self, arch='resnet18'):\n",
                "        super().__init__()\n",
                "        if arch == 'resnet18': model = models.resnet18(weights=None)\n",
                "        else: model = models.resnet34(weights=None)\n",
                "        model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
                "        model.fc = nn.Linear(model.fc.in_features, CFG.num_classes)\n",
                "        self.backbone, self.spec_layer = model, SpecTransform()\n",
                "    def forward(self, x): return self.backbone(self.spec_layer(x))\n",
                "\n",
                "class AudioEffNet(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        model = models.efficientnet_b0(weights=None)\n",
                "        old_conv = model.features[0][0]\n",
                "        model.features[0][0] = nn.Conv2d(1, old_conv.out_channels, kernel_size=old_conv.kernel_size, stride=old_conv.stride, padding=old_conv.padding, bias=False)\n",
                "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, CFG.num_classes)\n",
                "        self.backbone, self.spec_layer = model, SpecTransform()\n",
                "    def forward(self, x): return self.backbone(self.spec_layer(x))\n",
                "\n",
                "class TestDataset(Dataset):\n",
                "    def __init__(self, waveforms): self.waveforms = waveforms\n",
                "    def __len__(self): return len(self.waveforms)\n",
                "    def __getitem__(self, idx): return torch.from_numpy(self.waveforms[idx]).float()\n",
                "\n",
                "def get_ensemble_probs(waveforms):\n",
                "    loader = DataLoader(TestDataset(waveforms), batch_size=CFG.batch_size, shuffle=False)\n",
                "    all_probs = []\n",
                "    models_to_run = [(AudioResNet, 'best_model_fold'), (lambda: AudioResNet(arch='resnet34'), 'resnet34_fold'), (AudioEffNet, 'effnet_fold')]\n",
                "    for m_class, prefix in models_to_run:\n",
                "        for fold in range(5):\n",
                "            path = f'{MODEL_PATH}/{prefix}_{fold}.pth'\n",
                "            if not os.path.exists(path): continue\n",
                "            print(f\"Loading {prefix} Fold {fold}\")\n",
                "            m = m_class().to(device); m.load_state_dict(torch.load(path, map_location=device)); m.eval()\n",
                "            probs = []\n",
                "            with torch.no_grad():\n",
                "                for x in tqdm(loader): probs.append(F.softmax(m(x.to(device)), dim=1).cpu().numpy())\n",
                "            all_probs.append(np.concatenate(probs))\n",
                "    return np.mean(all_probs, axis=0)\n",
                "\n",
                "print(\"Predicting probabilities for test set...\")\n",
                "pub, priv = np.load(f'{DATA_DIR}/public_test_waveforms.npy'), np.load(f'{DATA_DIR}/private_test_waveforms.npy')\n",
                "test_waveforms = np.concatenate([pub, priv])\n",
                "test_probs = get_ensemble_probs(test_waveforms)\n",
                "\n",
                "confidences = np.max(test_probs, axis=1)\n",
                "pseudo_labels = np.argmax(test_probs, axis=1)\n",
                "mask = confidences > THRESHOLD\n",
                "\n",
                "print(f\"Found {np.sum(mask)} high-confidence samples\")\n",
                "\n",
                "train_wavs = np.load(f'{DATA_DIR}/train_waveforms.npy')\n",
                "train_labels = np.load(f'{DATA_DIR}/train_labels.npy')\n",
                "\n",
                "expanded_wavs = np.concatenate([train_wavs, test_waveforms[mask]])\n",
                "expanded_labels = np.concatenate([train_labels, pseudo_labels[mask]])\n",
                "\n",
                "np.save('expanded_train_waveforms.npy', expanded_wavs)\n",
                "np.save('expanded_train_labels.npy', expanded_labels)\n",
                "print(\"Expanded dataset saved!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}