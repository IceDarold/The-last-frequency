{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# The Last Frequency: ResNet-34 5-Fold Solution\n",
                "\n",
                "This notebook implements a deeper ResNet-34 architecture to capture more complex audio features."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, json, random, numpy as np, pandas as pd, torch, torch.nn as nn, torch.nn.functional as F, torchaudio, torchvision.models as models\n",
                "from tqdm.auto import tqdm\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "def seed_everything(seed=42):\n",
                "    random.seed(seed); os.environ['PYTHONHASHSEED'] = str(seed); np.random.seed(seed)\n",
                "    torch.manual_seed(seed); torch.cuda.manual_seed(seed)\n",
                "    torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
                "\n",
                "seed_everything(42)\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "\n",
                "class CFG:\n",
                "    data_dir = '/kaggle/input/the-last-frequency'\n",
                "    sample_rate, n_fft, hop_length, n_mels, target_frames = 16000, 1024, 256, 128, 64\n",
                "    n_splits, batch_size, epochs, lr, weight_decay, label_smoothing, mixup_alpha = 5, 64, 35, 1e-3, 1e-2, 0.1, 0.2\n",
                "    num_classes = 35\n",
                "\n",
                "train_waveforms = np.load(f'{CFG.data_dir}/train_waveforms.npy')\n",
                "train_labels = np.load(f'{CFG.data_dir}/train_labels.npy')\n",
                "with open(f'{CFG.data_dir}/label_map.json') as f: label_map = {int(k): v for k, v in json.load(f).items()}\n",
                "\n",
                "class SpecTransform(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.mel_spec = torchaudio.transforms.MelSpectrogram(sample_rate=CFG.sample_rate, n_fft=CFG.n_fft, hop_length=CFG.hop_length, n_mels=CFG.n_mels)\n",
                "        self.amp_to_db = torchaudio.transforms.AmplitudeToDB()\n",
                "        self.f_mask, self.t_mask = torchaudio.transforms.FrequencyMasking(20), torchaudio.transforms.TimeMasking(15)\n",
                "    def forward(self, x, augment=False):\n",
                "        x = self.amp_to_db(self.mel_spec(x))\n",
                "        if x.shape[-1] > CFG.target_frames: x = x[..., :CFG.target_frames]\n",
                "        elif x.shape[-1] < CFG.target_frames: x = F.pad(x, (0, CFG.target_frames - x.shape[-1]))\n",
                "        if augment: x = self.t_mask(self.f_mask(x))\n",
                "        return x\n",
                "\n",
                "class AudioResNet34(nn.Module):\n",
                "    def __init__(self, num_classes=35):\n",
                "        super().__init__()\n",
                "        model = models.resnet34(weights=None)\n",
                "        model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
                "        model.fc = nn.Sequential(nn.Dropout(0.3), nn.Linear(model.fc.in_features, num_classes))\n",
                "        self.backbone, self.spec_layer = model, SpecTransform()\n",
                "    def forward(self, x, augment=False):\n",
                "        return self.backbone(self.spec_layer(x, augment=augment).unsqueeze(1))\n",
                "\n",
                "class SpeechDataset(Dataset):\n",
                "    def __init__(self, waveforms, labels=None, augment=False):\n",
                "        self.waveforms, self.labels, self.augment = waveforms, labels, augment\n",
                "    def __len__(self): return len(self.waveforms)\n",
                "    def __getitem__(self, idx):\n",
                "        wav = self.waveforms[idx].copy()\n",
                "        if self.augment: wav = np.roll(wav, int(random.uniform(-0.1, 0.1) * wav.shape[0]))\n",
                "        wav = torch.from_numpy(wav).float()\n",
                "        return (wav, self.labels[idx]) if self.labels is not None else wav\n",
                "\n",
                "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=42)\n",
                "for fold, (train_idx, val_idx) in enumerate(skf.split(train_waveforms, train_labels)):\n",
                "    print(f\"Fold {fold+1}\")\n",
                "    train_loader = DataLoader(SpeechDataset(train_waveforms[train_idx], train_labels[train_idx], augment=True), batch_size=CFG.batch_size, shuffle=True, num_workers=2)\n",
                "    val_loader = DataLoader(SpeechDataset(train_waveforms[val_idx], train_labels[val_idx], augment=False), batch_size=CFG.batch_size, shuffle=False)\n",
                "    model = AudioResNet34(CFG.num_classes).to(device)\n",
                "    opt = torch.optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
                "    sched = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=CFG.lr*2, steps_per_epoch=len(train_loader), epochs=CFG.epochs)\n",
                "    crit = nn.CrossEntropyLoss(label_smoothing=CFG.label_smoothing)\n",
                "    best_acc = 0\n",
                "    for epoch in range(1, CFG.epochs + 1):\n",
                "        model.train()\n",
                "        for x, y in train_loader:\n",
                "            x, y = x.to(device), y.to(device)\n",
                "            p = model(x, augment=True)\n",
                "            l = crit(p, y)\n",
                "            opt.zero_grad(); l.backward(); opt.step(); sched.step()\n",
                "        model.eval(); vp, vt = [], []\n",
                "        with torch.no_grad():\n",
                "            for x, y in val_loader:\n",
                "                vp.extend(model(x.to(device)).argmax(1).cpu().numpy()); vt.extend(y.numpy())\n",
                "        acc = accuracy_score(vt, vp)\n",
                "        if acc > best_acc:\n",
                "            best_acc = acc; torch.save(model.state_dict(), f'resnet34_fold_{fold}.pth')\n",
                "        if epoch % 10 == 0: print(f\"Epoch {epoch} Val Acc: {acc:.4f}\")\n",
                "\n",
                "def get_probs(wavs):\n",
                "    all_p = []\n",
                "    ld = DataLoader(SpeechDataset(wavs, augment=False), batch_size=CFG.batch_size, shuffle=False)\n",
                "    for f in range(CFG.n_splits):\n",
                "        m = AudioResNet34(CFG.num_classes).to(device)\n",
                "        m.load_state_dict(torch.load(f'resnet34_fold_{f}.pth'))\n",
                "        m.eval(); fp = []\n",
                "        with torch.no_grad():\n",
                "            for x in tqdm(ld): fp.append(F.softmax(m(x.to(device)), dim=1).cpu().numpy())\n",
                "        all_p.append(np.concatenate(fp))\n",
                "    return np.mean(all_p, axis=0)\n",
                "\n",
                "pub, priv = np.load(f'{CFG.data_dir}/public_test_waveforms.npy'), np.load(f'{CFG.data_dir}/private_test_waveforms.npy')\n",
                "final_cmds = [label_map[i] for i in np.concatenate([get_probs(pub).argmax(1), get_probs(priv).argmax(1)])]\n",
                "pd.DataFrame({'Id': range(len(final_cmds)), 'Command': final_cmds}).to_csv('resnet34_submission.csv', index=False)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}