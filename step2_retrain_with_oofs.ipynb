{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Step 2: Retraining with OOF (Out-of-Fold) Predictions\n",
                "\n",
                "This notebook trains a specific architecture on the **expanded dataset** (original + pseudo-labels) and saves OOF probabilities for the next stacking step.\n",
                "\n",
                "**Note**: You should run this notebook 3 times, changing the `ARCH` variable to `'resnet18'`, `'resnet34'`, and `'efficientnet_b0'` respectively."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, json, random, numpy as np, pandas as pd, torch, torch.nn as nn, torch.nn.functional as F, torchaudio, torchvision.models as models\n",
                "from tqdm.auto import tqdm\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.metrics import accuracy_score\n",
                "\n",
                "# ================= CONFIGURATION =================\n",
                "ARCH = 'resnet18' # Options: 'resnet18', 'resnet34', 'efficientnet_b0'\n",
                "DATA_DIR = '/kaggle/input/the-last-frequency'\n",
                "PSEUDO_DATA_DIR = '/kaggle/working' \n",
                "\n",
                "class CFG:\n",
                "    sample_rate, n_fft, hop_length, n_mels, target_frames = 16000, 1024, 256, 128, 64\n",
                "    n_splits, batch_size, epochs, lr, weight_decay, label_smoothing, mixup_alpha = 5, 64, 35, 1e-3, 1e-2, 0.1, 0.2\n",
                "    num_classes = 35\n",
                "\n",
                "def seed_everything(seed=42):\n",
                "    random.seed(seed); os.environ['PYTHONHASHSEED'] = str(seed); np.random.seed(seed)\n",
                "    torch.manual_seed(seed); torch.cuda.manual_seed(seed)\n",
                "    torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
                "\n",
                "seed_everything(42)\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "# =================================================\n",
                "\n",
                "print(f\"Loading expanded data for {ARCH}...\")\n",
                "train_waveforms = np.load(f'{PSEUDO_DATA_DIR}/expanded_train_waveforms.npy')\n",
                "train_labels = np.load(f'{PSEUDO_DATA_DIR}/expanded_train_labels.npy')\n",
                "with open(f'{DATA_DIR}/label_map.json') as f: \n",
                "    label_map = {int(k): v for k, v in json.load(f).items()}\n",
                "\n",
                "class SpecTransform(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.mel_spec = torchaudio.transforms.MelSpectrogram(sample_rate=CFG.sample_rate, n_fft=CFG.n_fft, hop_length=CFG.hop_length, n_mels=CFG.n_mels)\n",
                "        self.amp_to_db = torchaudio.transforms.AmplitudeToDB()\n",
                "        self.f_mask, self.t_mask = torchaudio.transforms.FrequencyMasking(20), torchaudio.transforms.TimeMasking(15)\n",
                "    def forward(self, x, augment=False):\n",
                "        x = self.amp_to_db(self.mel_spec(x))\n",
                "        if x.shape[-1] > CFG.target_frames: x = x[..., :CFG.target_frames]\n",
                "        elif x.shape[-1] < CFG.target_frames: x = F.pad(x, (0, CFG.target_frames - x.shape[-1]))\n",
                "        if augment: x = self.t_mask(self.f_mask(x))\n",
                "        return x\n",
                "\n",
                "def get_model(arch):\n",
                "    if arch == 'resnet18': model = models.resnet18(weights=None)\n",
                "    elif arch == 'resnet34': model = models.resnet34(weights=None)\n",
                "    elif arch == 'efficientnet_b0': model = models.efficientnet_b0(weights=None)\n",
                "    if 'resnet' in arch:\n",
                "        model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
                "        model.fc = nn.Sequential(nn.Dropout(0.3), nn.Linear(model.fc.in_features, CFG.num_classes))\n",
                "    else: \n",
                "        old_conv = model.features[0][0]\n",
                "        model.features[0][0] = nn.Conv2d(1, old_conv.out_channels, old_conv.kernel_size, old_conv.stride, old_conv.padding, bias=False)\n",
                "        model.classifier[1] = nn.Sequential(nn.Dropout(0.3), nn.Linear(model.classifier[1].in_features, CFG.num_classes))\n",
                "    class Wrapper(nn.Module):\n",
                "        def __init__(self, backbone):\n",
                "            super().__init__()\n",
                "            self.backbone, self.spec_layer = backbone, SpecTransform()\n",
                "        def forward(self, x, augment=False):\n",
                "            return self.backbone(self.spec_layer(x, augment=augment).unsqueeze(1))\n",
                "    return Wrapper(model).to(device)\n",
                "\n",
                "class SpeechDataset(Dataset):\n",
                "    def __init__(self, waveforms, labels=None, augment=False):\n",
                "        self.waveforms, self.labels, self.augment = waveforms, labels, augment\n",
                "    def __len__(self): return len(self.waveforms)\n",
                "    def __getitem__(self, idx):\n",
                "        wav = self.waveforms[idx].copy()\n",
                "        if self.augment: wav = np.roll(wav, int(random.uniform(-0.1, 0.1) * wav.shape[0]))\n",
                "        wav = torch.from_numpy(wav).float()\n",
                "        return (wav, self.labels[idx]) if self.labels is not None else wav\n",
                "\n",
                "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=42)\n",
                "oof_probs = np.zeros((len(train_labels), CFG.num_classes))\n",
                "pub, priv = np.load(f'{DATA_DIR}/public_test_waveforms.npy'), np.load(f'{DATA_DIR}/private_test_waveforms.npy')\n",
                "test_wavs = np.concatenate([pub, priv])\n",
                "all_test_probs = []\n",
                "for fold, (train_idx, val_idx) in enumerate(skf.split(train_waveforms, train_labels)):\n",
                "    print(f\"Fold {fold+1}/{CFG.n_splits}\")\n",
                "    train_loader = DataLoader(SpeechDataset(train_waveforms[train_idx], train_labels[train_idx], augment=True), batch_size=CFG.batch_size, shuffle=True)\n",
                "    val_loader = DataLoader(SpeechDataset(train_waveforms[val_idx], train_labels[val_idx], augment=False), batch_size=CFG.batch_size, shuffle=False)\n",
                "    model = get_model(ARCH)\n",
                "    opt = torch.optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
                "    sched = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=CFG.lr*2, steps_per_epoch=len(train_loader), epochs=CFG.epochs)\n",
                "    crit = nn.CrossEntropyLoss(label_smoothing=CFG.label_smoothing)\n",
                "    best_acc = 0\n",
                "    for epoch in range(1, CFG.epochs + 1):\n",
                "        model.train()\n",
                "        for x, y in train_loader: \n",
                "            p = model(x.to(device), augment=True); l = crit(p, y.to(device))\n",
                "            opt.zero_grad(); l.backward(); opt.step(); sched.step()\n",
                "        model.eval(); vp, vt = [], []\n",
                "        with torch.no_grad():\n",
                "            for x, y in val_loader:\n",
                "                out = model(x.to(device)); vp.append(F.softmax(out, dim=1).cpu().numpy()); vt.extend(y.numpy())\n",
                "        vp = np.concatenate(vp); acc = accuracy_score(vt, vp.argmax(1))\n",
                "        if acc > best_acc: \n",
                "            best_acc = acc; torch.save(model.state_dict(), f'{ARCH}_fold_{fold}.pth'); oof_probs[val_idx] = vp\n",
                "        if epoch % 10 == 0: print(f\"Epoch {epoch} Val Acc: {acc:.4f}\")\n",
                "    model.load_state_dict(torch.load(f'{ARCH}_fold_{fold}.pth')); model.eval(); tp = []\n",
                "    test_loader = DataLoader(SpeechDataset(test_wavs, augment=False), batch_size=CFG.batch_size, shuffle=False)\n",
                "    with torch.no_grad():\n",
                "        for x in tqdm(test_loader): tp.append(F.softmax(model(x.to(device)), dim=1).cpu().numpy())\n",
                "    all_test_probs.append(np.concatenate(tp))\n",
                "np.save(f'{ARCH}_oof_probs.npy', oof_probs)\n",
                "np.save(f'{ARCH}_test_probs.npy', np.mean(all_test_probs, axis=0))\n",
                "print(f\"Saved results for {ARCH}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}