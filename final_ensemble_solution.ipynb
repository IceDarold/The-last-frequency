{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# The Last Frequency: Final Winning Ensemble\n",
                "\n",
                "This notebook combines 15 models (5 x ResNet-18, 5 x ResNet-34, 5 x EfficientNet-B0) to achieve the highest possible accuracy on the leaderboard.\n",
                "\n",
                "### Strategy:\n",
                "1. **Load weights** for all folds of all three architectures.\n",
                "2. **Predict Probabilities**: Calculate Softmax probabilities for each model on the test sets.\n",
                "3. **Weighted Averaging**: Combine the probabilities (Simple Mean or Weighted Mean).\n",
                "4. **Final Decision**: Take the `argmax` of the averaged probabilities."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, json, numpy as np, pandas as pd, torch, torch.nn as nn, torch.nn.functional as F, torchaudio, torchvision.models as models\n",
                "from tqdm.auto import tqdm\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "MODEL_PATH = '/kaggle/input/the-last-frequency-models'\n",
                "DATA_DIR = '/kaggle/input/the-last-frequency'\n",
                "\n",
                "class CFG:\n",
                "    sample_rate, n_fft, hop_length, n_mels, target_frames = 16000, 1024, 256, 128, 64\n",
                "    batch_size = 128\n",
                "    num_classes = 35\n",
                "\n",
                "with open(f'{DATA_DIR}/label_map.json') as f: \n",
                "    label_map = {int(k): v for k, v in json.load(f).items()}\n",
                "\n",
                "class SpecTransform(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.mel_spec = torchaudio.transforms.MelSpectrogram(sample_rate=CFG.sample_rate, n_fft=CFG.n_fft, hop_length=CFG.hop_length, n_mels=CFG.n_mels)\n",
                "        self.amp_to_db = torchaudio.transforms.AmplitudeToDB()\n",
                "    def forward(self, x):\n",
                "        x = self.amp_to_db(self.mel_spec(x))\n",
                "        if x.shape[-1] > CFG.target_frames: x = x[..., :CFG.target_frames]\n",
                "        elif x.shape[-1] < CFG.target_frames: x = F.pad(x, (0, CFG.target_frames - x.shape[-1]))\n",
                "        return x.unsqueeze(1)\n",
                "\n",
                "class AudioResNet(nn.Module):\n",
                "    def __init__(self, arch='resnet18'):\n",
                "        super().__init__()\n",
                "        if arch == 'resnet18': model = models.resnet18(weights=None)\n",
                "        else: model = models.resnet34(weights=None)\n",
                "        model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
                "        model.fc = nn.Linear(model.fc.in_features, CFG.num_classes)\n",
                "        self.backbone, self.spec_layer = model, SpecTransform()\n",
                "    def forward(self, x): return self.backbone(self.spec_layer(x))\n",
                "\n",
                "class AudioEffNet(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        model = models.efficientnet_b0(weights=None)\n",
                "        old_conv = model.features[0][0]\n",
                "        model.features[0][0] = nn.Conv2d(1, old_conv.out_channels, kernel_size=old_conv.kernel_size, stride=old_conv.stride, padding=old_conv.padding, bias=False)\n",
                "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, CFG.num_classes)\n",
                "        self.backbone, self.spec_layer = model, SpecTransform()\n",
                "    def forward(self, x): return self.backbone(self.spec_layer(x))\n",
                "\n",
                "class TestDataset(Dataset):\n",
                "    def __init__(self, waveforms): self.waveforms = waveforms\n",
                "    def __len__(self): return len(self.waveforms)\n",
                "    def __getitem__(self, idx): return torch.from_numpy(self.waveforms[idx]).float()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Inference Loop"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_probs(waveforms):\n",
                "    loader = DataLoader(TestDataset(waveforms), batch_size=CFG.batch_size, shuffle=False)\n",
                "    all_probs = []\n",
                "    \n",
                "    # Architecture config: (class, weight, prefix)\n",
                "    models_to_run = [\n",
                "        (AudioResNet, 1.0, 'best_model_fold'),      # ResNet-18\n",
                "        (lambda: AudioResNet(arch='resnet34'), 1.2, 'resnet34_fold'), # ResNet-34 (higher weight)\n",
                "        (AudioEffNet, 1.1, 'effnet_fold')           # EffNet-B0\n",
                "    ]\n",
                "    \n",
                "    for model_class, weight, prefix in models_to_run:\n",
                "        for fold in range(5):\n",
                "            path = f'{MODEL_PATH}/{prefix}_{fold}.pth'\n",
                "            if not os.path.exists(path):\n",
                "                print(f\"Warning: {path} not found, skipping.\")\n",
                "                continue\n",
                "            \n",
                "            print(f\"Predicting with {prefix} Fold {fold} (Weight: {weight})...\")\n",
                "            model = model_class().to(device)\n",
                "            model.load_state_dict(torch.load(path, map_location=device))\n",
                "            model.eval()\n",
                "            \n",
                "            probs = []\n",
                "            with torch.no_grad():\n",
                "                for x in tqdm(loader):\n",
                "                    out = model(x.to(device))\n",
                "                    probs.append(F.softmax(out, dim=1).cpu().numpy() * weight)\n",
                "            \n",
                "            all_probs.append(np.concatenate(probs))\n",
                "            \n",
                "    return np.sum(all_probs, axis=0)\n",
                "\n",
                "print(\"Loading test data...\")\n",
                "pub = np.load(f'{DATA_DIR}/public_test_waveforms.npy')\n",
                "priv = np.load(f'{DATA_DIR}/private_test_waveforms.npy')\n",
                "\n",
                "print(\"Starting Grand Ensemble Inference...\")\n",
                "final_pub_probs = get_probs(pub)\n",
                "final_priv_probs = get_probs(priv)\n",
                "\n",
                "final_indices = np.concatenate([final_pub_probs.argmax(1), final_priv_probs.argmax(1)])\n",
                "final_cmds = [label_map[idx] for idx in final_indices]\n",
                "\n",
                "submission = pd.DataFrame({'Id': range(len(final_cmds)), 'Command': final_cmds})\n",
                "submission.to_csv('submission.csv', index=False)\n",
                "print(\"\\nFinal submission.csv saved!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
