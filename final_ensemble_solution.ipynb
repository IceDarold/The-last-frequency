{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# The Last Frequency: Final Winning Ensemble (Fixed Architecture)\n",
                "\n",
                "This notebook combines 15 models (5 x ResNet-18, 5 x ResNet-34, 5 x EfficientNet-B0) with 3-channel input support and matching head layers."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, json, numpy as np, pandas as pd, torch, torch.nn as nn, torch.nn.functional as F, torchaudio, torchvision.models as models\n",
                "from tqdm.auto import tqdm\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "MODEL_PATH = '/kaggle/input/the-last-frequency-models'\n",
                "DATA_DIR = '/kaggle/input/the-last-frequency'\n",
                "\n",
                "class CFG:\n",
                "    sample_rate, n_fft, hop_length, n_mels, target_frames = 16000, 1024, 256, 128, 64\n",
                "    batch_size = 128\n",
                "    num_classes = 35\n",
                "\n",
                "with open(f'{DATA_DIR}/label_map.json') as f: \n",
                "    label_map = {int(k): v for k, v in json.load(f).items()}\n",
                "\n",
                "class SpecTransform(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.mel_spec = torchaudio.transforms.MelSpectrogram(sample_rate=CFG.sample_rate, n_fft=CFG.n_fft, hop_length=CFG.hop_length, n_mels=CFG.n_mels)\n",
                "        self.amp_to_db = torchaudio.transforms.AmplitudeToDB()\n",
                "    def forward(self, x):\n",
                "        spec = self.amp_to_db(self.mel_spec(x))\n",
                "        if spec.shape[-1] > CFG.target_frames: spec = spec[..., :CFG.target_frames]\n",
                "        elif spec.shape[-1] < CFG.target_frames: spec = F.pad(spec, (0, CFG.target_frames - spec.shape[-1]))\n",
                "        \n",
                "        # Multi-channel logic needed for SOTA models\n",
                "        delta = torchaudio.functional.compute_deltas(spec)\n",
                "        delta2 = torchaudio.functional.compute_deltas(delta)\n",
                "        return torch.stack([spec, delta, delta2], dim=1)\n",
                "\n",
                "class AudioResNet(nn.Module):\n",
                "    def __init__(self, arch='resnet18'):\n",
                "        super().__init__()\n",
                "        if arch == 'resnet18': model = models.resnet18(weights=None)\n",
                "        else: model = models.resnet34(weights=None)\n",
                "        \n",
                "        # MATCHING TRAINING ARCHITECTURE (Dropout + Linear)\n",
                "        model.fc = nn.Sequential(\n",
                "            nn.Dropout(0.3),\n",
                "            nn.Linear(model.fc.in_features, CFG.num_classes)\n",
                "        )\n",
                "        self.backbone, self.spec_layer = model, SpecTransform()\n",
                "    def forward(self, x): \n",
                "        return self.backbone(self.spec_layer(x))\n",
                "\n",
                "class AudioEffNet(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        model = models.efficientnet_b0(weights=None)\n",
                "        # Check if your training used 1 or 3 channels. \n",
                "        # If you used the ULTIMATE script, it's 3 channels.\n",
                "        old_conv = model.features[0][0]\n",
                "        model.features[0][0] = nn.Conv2d(3, old_conv.out_channels, kernel_size=old_conv.kernel_size, stride=old_conv.stride, padding=old_conv.padding, bias=False)\n",
                "        \n",
                "        model.classifier[1] = nn.Sequential(\n",
                "            nn.Dropout(0.3),\n",
                "            nn.Linear(model.classifier[1].in_features, CFG.num_classes)\n",
                "        )\n",
                "        self.backbone, self.spec_layer = model, SpecTransform()\n",
                "    def forward(self, x): return self.backbone(self.spec_layer(x))\n",
                "\n",
                "class TestDataset(Dataset):\n",
                "    def __init__(self, waveforms): self.waveforms = waveforms\n",
                "    def __len__(self): return len(self.waveforms)\n",
                "    def __getitem__(self, idx): return torch.from_numpy(self.waveforms[idx]).float()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_probs(waveforms):\n",
                "    loader = DataLoader(TestDataset(waveforms), batch_size=CFG.batch_size, shuffle=False)\n",
                "    all_probs = []\n",
                "    \n",
                "    # Architecture config: (factory, weight, prefix)\n",
                "    models_to_run = [\n",
                "        (lambda: AudioResNet(arch='resnet18'), 1.0, 'best_model_fold'),      # ResNet-18\n",
                "        (lambda: AudioResNet(arch='resnet34'), 1.2, 'resnet34_fold'), # ResNet-34\n",
                "        (AudioEffNet, 1.1, 'effnet_fold')           # EffNet-B0\n",
                "    ]\n",
                "    \n",
                "    for model_factory, weight, prefix in models_to_run:\n",
                "        for fold in range(5):\n",
                "            path = f'{MODEL_PATH}/{prefix}_{fold}.pth'\n",
                "            if not os.path.exists(path):\n",
                "                print(f\"Warning: {path} not found.\")\n",
                "                continue\n",
                "            \n",
                "            print(f\"Predicting {prefix} Fold {fold}...\")\n",
                "            model = model_factory().to(device)\n",
                "            model.load_state_dict(torch.load(path, map_location=device))\n",
                "            model.eval()\n",
                "            \n",
                "            probs = []\n",
                "            with torch.no_grad():\n",
                "                for x in tqdm(loader, leave=False):\n",
                "                    out = model(x.to(device))\n",
                "                    probs.append(F.softmax(out, dim=1).cpu().numpy() * weight)\n",
                "            all_probs.append(np.concatenate(probs))\n",
                "            \n",
                "    return np.sum(all_probs, axis=0)\n",
                "\n",
                "print(\"Loading test data...\")\n",
                "pub = np.load(f'{DATA_DIR}/public_test_waveforms.npy')\n",
                "priv = np.load(f'{DATA_DIR}/private_test_waveforms.npy')\n",
                "\n",
                "final_pub_probs = get_probs(pub)\n",
                "final_priv_probs = get_probs(priv)\n",
                "\n",
                "final_indices = np.concatenate([final_pub_probs.argmax(1), final_priv_probs.argmax(1)])\n",
                "final_cmds = [label_map[idx] for idx in final_indices]\n",
                "\n",
                "submission = pd.DataFrame({'Id': range(len(final_cmds)), 'Command': final_cmds})\n",
                "submission.to_csv('submission.csv', index=False)\n",
                "print(\"\\nFixed Ensemble Submission Saved!\")"
            ]
        }
    ],
    "metadata": { \"kernelspec\": { \"display_name\": \"Python 3\", \"name\": \"python3\" } },"nbformat": 4,
        "nbformat_minor": 5
    }