{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# The Last Frequency: Final Winning Ensemble (1-Channel Fix)\n",
                "\n",
                "Этот блокнот настроен на работу с твоими текущими 1-канальными весами. \n",
                "Если ты переобучишь модели с Delta-признаками, нужно будет вернуть 3 канала."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, json, numpy as np, pandas as pd, torch, torch.nn as nn, torch.nn.functional as F, torchaudio, torchvision.models as models\n",
                "from tqdm.auto import tqdm\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "MODEL_PATH = '/kaggle/input/the-last-frequency-models'\n",
                "DATA_DIR = '/kaggle/input/the-last-frequency'\n",
                "\n",
                "class CFG:\n",
                "    sample_rate, n_fft, hop_length, n_mels, target_frames = 16000, 1024, 256, 128, 64\n",
                "    batch_size = 128\n",
                "    num_classes = 35\n",
                "    in_channels = 1 # МЕНЯЕМ НА 1, чтобы совпало с твоими весами\n",
                "\n",
                "with open(f'{DATA_DIR}/label_map.json') as f: \n",
                "    label_map = {int(k): v for k, v in json.load(f).items()}\n",
                "\n",
                "class SpecTransform(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.mel_spec = torchaudio.transforms.MelSpectrogram(sample_rate=CFG.sample_rate, n_fft=CFG.n_fft, hop_length=CFG.hop_length, n_mels=CFG.n_mels)\n",
                "        self.amp_to_db = torchaudio.transforms.AmplitudeToDB()\n",
                "    def forward(self, x):\n",
                "        spec = self.amp_to_db(self.mel_spec(x))\n",
                "        if spec.shape[-1] > CFG.target_frames: spec = spec[..., :CFG.target_frames]\n",
                "        elif spec.shape[-1] < CFG.target_frames: spec = F.pad(spec, (0, CFG.target_frames - spec.shape[-1]))\n",
                "        return spec.unsqueeze(1) # Возвращаем (B, 1, F, T)\n",
                "\n",
                "class AudioResNet(nn.Module):\n",
                "    def __init__(self, arch='resnet18'):\n",
                "        super().__init__()\n",
                "        if arch == 'resnet18': model = models.resnet18(weights=None)\n",
                "        else: model = models.resnet34(weights=None)\n",
                "        \n",
                "        # ФИКС: Устанавливаем 1 входной канал для conv1, чтобы совпало с весами [64, 1, 7, 7]\n",
                "        model.conv1 = nn.Conv2d(CFG.in_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
                "        \n",
                "        model.fc = nn.Sequential(\n",
                "            nn.Dropout(0.3),\n",
                "            nn.Linear(model.fc.in_features, CFG.num_classes)\n",
                "        )\n",
                "        self.backbone, self.spec_layer = model, SpecTransform()\n",
                "    def forward(self, x): return self.backbone(self.spec_layer(x))\n",
                "\n",
                "class AudioEffNet(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        model = models.efficientnet_b0(weights=None)\n",
                "        # ФИКС: 1 канал для EffNet\n",
                "        old_conv = model.features[0][0]\n",
                "        model.features[0][0] = nn.Conv2d(CFG.in_channels, old_conv.out_channels, 3, stride=2, padding=1, bias=False)\n",
                "        \n",
                "        model.classifier[1] = nn.Sequential(\n",
                "            nn.Dropout(0.3),\n",
                "            nn.Linear(model.classifier[1].in_features, CFG.num_classes)\n",
                "        )\n",
                "        self.backbone, self.spec_layer = model, SpecTransform()\n",
                "    def forward(self, x): return self.backbone(self.spec_layer(x))\n",
                "\n",
                "class TestDataset(Dataset):\n",
                "    def __init__(self, waveforms): self.waveforms = waveforms\n",
                "    def __len__(self): return len(self.waveforms)\n",
                "    def __getitem__(self, idx): return torch.from_numpy(self.waveforms[idx]).float()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_probs(waveforms):\n",
                "    loader = DataLoader(TestDataset(waveforms), batch_size=CFG.batch_size, shuffle=False)\n",
                "    all_probs = []\n",
                "    \n",
                "    models_to_run = [\n",
                "        (lambda: AudioResNet(arch='resnet18'), 1.0, 'best_model_fold'), \n",
                "        (lambda: AudioResNet(arch='resnet34'), 1.2, 'resnet34_fold'), \n",
                "        (AudioEffNet, 1.1, 'effnet_fold')\n",
                "    ]\n",
                "    \n",
                "    for model_factory, weight, prefix in models_to_run:\n",
                "        for fold in range(5):\n",
                "            path = f'{MODEL_PATH}/{prefix}_{fold}.pth'\n",
                "            if not os.path.exists(path): continue\n",
                "            \n",
                "            print(f\"Predicting with {prefix}_{fold}...\")\n",
                "            model = model_factory().to(device)\n",
                "            model.load_state_dict(torch.load(path, map_location=device))\n",
                "            model.eval()\n",
                "            \n",
                "            probs = []\n",
                "            with torch.no_grad():\n",
                "                for x in tqdm(loader, leave=False): \n",
                "                    out = model(x.to(device))\n",
                "                    probs.append(F.softmax(out, dim=1).cpu().numpy() * weight)\n",
                "            all_probs.append(np.concatenate(probs))\n",
                "            \n",
                "    return np.sum(all_probs, axis=0)\n",
                "\n",
                "print(\"Loading test data...\")\n",
                "pub = np.load(f'{DATA_DIR}/public_test_waveforms.npy')\n",
                "priv = np.load(f'{DATA_DIR}/private_test_waveforms.npy')\n",
                "\n",
                "print(\"Starting inference...\")\n",
                "final_pub_probs = get_probs(pub)\n",
                "final_priv_probs = get_probs(priv)\n",
                "\n",
                "final_indices = np.concatenate([final_pub_probs.argmax(1), final_priv_probs.argmax(1)])\n",
                "final_cmds = [label_map[idx] for idx in final_indices]\n",
                "\n",
                "pd.DataFrame({'Id': range(len(final_cmds)), 'Command': final_cmds}).to_csv('submission.csv', index=False)\n",
                "print(\"Done! Fixed submission.csv ready.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}