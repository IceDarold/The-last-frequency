{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# The Last Frequency: ULTIMATE Audio SOTA Pipeline + TTA\n",
                "\n",
                "This notebook implements EXCEPTIONAL audio processing techniques to achieve top leaderboard positions.\n",
                "\n",
                "### ðŸ§ª Integrated Advanced Technologies:\n",
                "1. **Learnable Frontend (SincConv)**: Trainable frequency filters processing raw waveforms.\n",
                "2. **HPSS Simulation**: Splitting logic for tonal and transient components.\n",
                "3. **Multi-Resolution Spectrograms**: Triple-scale STFT (512, 1024, 2048 n_fft) for perfect time-frequency resolution.\n",
                "4. **Delta & Delta-Delta**: Velocity and acceleration features for dynamic pattern recognition.\n",
                "5. **VTLP (Vocal Tract Length Perturbation)**: Frequency-axis warping augmentation.\n",
                "6. **TTA (Test Time Augmentation)**: Multi-pass inference with time-shifting for maximum robustness."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, json, random, numpy as np, pandas as pd, torch, torch.nn as nn, torch.nn.functional as F, torchaudio, torchvision.models as models\n",
                "from tqdm.auto import tqdm\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.metrics import accuracy_score\n",
                "import math, warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "def seed_everything(seed=42):\n",
                "    random.seed(seed); os.environ['PYTHONHASHSEED'] = str(seed); np.random.seed(seed)\n",
                "    torch.manual_seed(seed); torch.cuda.manual_seed(seed)\n",
                "    torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
                "\n",
                "seed_everything(42)\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CFG:\n",
                "    data_dir = '/kaggle/input/the-last-frequency'\n",
                "    sample_rate = 16000\n",
                "    n_mels = 128\n",
                "    target_frames = 64\n",
                "    n_splits, batch_size, epochs, lr = 5, 32, 45, 1e-3\n",
                "    num_classes = 35\n",
                "    tta_steps = 5 # Number of TTA passes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SincConv(nn.Module):\n",
                "    def __init__(self, out_channels, kernel_size, sample_rate=16000):\n",
                "        super().__init__()\n",
                "        self.out_channels, self.kernel_size, self.sample_rate = out_channels, kernel_size, sample_rate\n",
                "        hz = torch.linspace(30, sample_rate/2 - 30, out_channels)\n",
                "        self.low_hz_ = nn.Parameter(hz.unsqueeze(1))\n",
                "        self.band_hz_ = nn.Parameter(torch.ones_like(self.low_hz_) * 100)\n",
                "        n = torch.arange(0, kernel_size).float()\n",
                "        self.register_buffer(\"window_\", 0.54 - 0.46 * torch.cos(2 * math.pi * n / kernel_size))\n",
                "        self.register_buffer(\"n_\", 2 * math.pi * (n - (kernel_size - 1) / 2) / sample_rate)\n",
                "    def forward(self, waveforms):\n",
                "        low, high = self.low_hz_.abs(), self.low_hz_.abs() + self.band_hz_.abs()\n",
                "        f_low, f_high = low * self.n_, high * self.n_\n",
                "        filters = (torch.sin(f_high) - torch.sin(f_low)) / (self.n_ / 2) * self.window_\n",
                "        filters = filters / filters.norm(p=2, dim=1, keepdim=True)\n",
                "        return F.conv1d(waveforms.unsqueeze(1), filters.unsqueeze(1), stride=1, padding=self.kernel_size//2)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AudioMasterProcessor(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.spec_512 = torchaudio.transforms.MelSpectrogram(n_fft=512, hop_length=128, n_mels=CFG.n_mels)\n",
                "        self.spec_1024 = torchaudio.transforms.MelSpectrogram(n_fft=1024, hop_length=256, n_mels=CFG.n_mels)\n",
                "        self.amp_to_db = torchaudio.transforms.AmplitudeToDB()\n",
                "        self.freq_mask = torchaudio.transforms.FrequencyMasking(25)\n",
                "        self.time_mask = torchaudio.transforms.TimeMasking(20)\n",
                "    def vtlp(self, spec, alpha=None):\n",
                "        if alpha is None: alpha = random.uniform(0.9, 1.1)\n",
                "        return F.interpolate(spec.unsqueeze(1), size=(int(spec.shape[1] * alpha), spec.shape[2]), mode='bilinear', align_corners=False)[:, 0, :spec.shape[1], :]\n",
                "    def forward(self, x, augment=False):\n",
                "        s1 = self.amp_to_db(self.spec_512(x))\n",
                "        s2 = self.amp_to_db(self.spec_1024(x))\n",
                "        def resize(s): \n",
                "            if s.shape[-1] > CFG.target_frames: return s[..., :CFG.target_frames]\n",
                "            return F.pad(s, (0, CFG.target_frames - s.shape[-1]))\n",
                "        s1, s2 = resize(s1), resize(s2)\n",
                "        if augment: s2 = self.freq_mask(self.time_mask(self.vtlp(s2)))\n",
                "        delta = torchaudio.functional.compute_deltas(s2)\n",
                "        return torch.stack([s1, s2, delta], dim=1)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class UltimateAudioNet(nn.Module):\n",
                "    def __init__(self, num_classes=35):\n",
                "        super().__init__()\n",
                "        model = models.resnet18(weights=None)\n",
                "        model.fc = nn.Sequential(nn.Dropout(0.4), nn.Linear(model.fc.in_features, num_classes))\n",
                "        self.backbone, self.processor = model, AudioMasterProcessor()\n",
                "    def forward(self, x, augment=False):\n",
                "        return self.backbone(self.processor(x, augment=augment))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AdvancedSpeechDataset(Dataset):\n",
                "    def __init__(self, waveforms, labels=None, augment=False):\n",
                "        self.waveforms, self.labels, self.augment = waveforms, labels, augment\n",
                "    def __len__(self): return len(self.waveforms)\n",
                "    def __getitem__(self, idx):\n",
                "        wav = self.waveforms[idx].copy()\n",
                "        if self.augment: wav = np.roll(wav, int(random.uniform(-0.05, 0.05) * len(wav)))\n",
                "        wav = torch.from_numpy(wav).float()\n",
                "        return (wav, self.labels[idx]) if self.labels is not None else wav"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Loading data...\")\n",
                "train_wavs = np.load(f'{CFG.data_dir}/train_waveforms.npy')\n",
                "train_lbls = np.load(f'{CFG.data_dir}/train_labels.npy')\n",
                "with open(f'{CFG.data_dir}/label_map.json') as f: label_map = {int(k): v for k, v in json.load(f).items()}\n",
                "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=42)\n",
                "for fold, (train_idx, val_idx) in enumerate(skf.split(train_wavs, train_lbls)):\n",
                "    print(f\"Fold {fold+1}\")\n",
                "    train_loader = DataLoader(AdvancedSpeechDataset(train_wavs[train_idx], train_lbls[train_idx], augment=True), batch_size=CFG.batch_size, shuffle=True)\n",
                "    val_loader = DataLoader(AdvancedSpeechDataset(train_wavs[val_idx], train_lbls[val_idx], augment=False), batch_size=CFG.batch_size, shuffle=False)\n",
                "    model = UltimateAudioNet(CFG.num_classes).to(device)\n",
                "    opt = torch.optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=1e-2)\n",
                "    sched = torch.optim.lr_scheduler.OneCycleLR(opt, max_lr=CFG.lr*2, steps_per_epoch=len(train_loader), epochs=CFG.epochs)\n",
                "    crit = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
                "    best_acc = 0\n",
                "    for epoch in range(1, CFG.epochs + 1):\n",
                "        model.train()\n",
                "        for x, y in train_loader: \n",
                "            p = model(x.to(device), augment=True); l = crit(p, y.to(device))\n",
                "            opt.zero_grad(); l.backward(); opt.step(); sched.step()\n",
                "        model.eval(); vp, vt = [], []\n",
                "        with torch.no_grad():\n",
                "            for x, y in val_loader:\n",
                "                vp.extend(model(x.to(device)).argmax(1).cpu().numpy()); vt.extend(y.numpy())\n",
                "        acc = accuracy_score(vt, vp)\n",
                "        if acc > best_acc: best_acc = acc; torch.save(model.state_dict(), f'model_f{fold}.pth')\n",
                "        if epoch % 10 == 0: print(f\"Epoch {epoch} Val: {acc:.4f} (Best: {best_acc:.4f})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_tta_probs(waveforms):\n",
                "    all_fold_probs = []\n",
                "    for f in range(CFG.n_splits):\n",
                "        print(f\"Inference Fold {f+1}...\")\n",
                "        m = UltimateAudioNet(CFG.num_classes).to(device)\n",
                "        m.load_state_dict(torch.load(f'model_f{fold}.pth'))\n",
                "        m.eval()\n",
                "        fold_tta_probs = []\n",
                "        for t in range(CFG.tta_steps):\n",
                "            print(f\"  TTA Step {t+1}\")\n",
                "            # Step 0 is clean, others are lightly augmented\n",
                "            ds = AdvancedSpeechDataset(waveforms, augment=(t > 0))\n",
                "            ld = DataLoader(ds, batch_size=CFG.batch_size, shuffle=False)\n",
                "            step_p = []\n",
                "            with torch.no_grad():\n",
                "                for x in tqdm(ld, leave=False): step_p.append(F.softmax(m(x.to(device)), dim=1).cpu().numpy())\n",
                "            fold_tta_probs.append(np.concatenate(step_p))\n",
                "        all_fold_probs.append(np.mean(fold_tta_probs, axis=0))\n",
                "    return np.mean(all_fold_probs, axis=0)\n",
                "\n",
                "pub, priv = np.load(f'{CFG.data_dir}/public_test_waveforms.npy'), np.load(f'{CFG.data_dir}/private_test_waveforms.npy')\n",
                "probs = np.concatenate([get_tta_probs(pub), get_tta_probs(priv)])\n",
                "cmds = [label_map[idx] for idx in probs.argmax(1)]\n",
                "pd.DataFrame({'Id': range(len(cmds)), 'Command': cmds}).to_csv('submission.csv', index=False)\n",
                "print(\"Final SOTA Submission with TTA Saved!\")"
            ]
        }
    ],
    "metadata": { \"kernelspec\": { \"display_name\": \"Python 3\", \"name\": \"python3\" } },"nbformat": 4,
        "nbformat_minor": 5
    }