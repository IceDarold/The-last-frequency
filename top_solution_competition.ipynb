{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# The Last Frequency: Audio SOTA K-Fold Solution\n",
                "\n",
                "This notebook implements a high-performance audio classification pipeline with **Delta & Delta-Delta features**, transforming a grayscale spectrogram into a 3-channel (RGB-like) representation for ResNet.\n",
                "\n",
                "### SOTA Features:\n",
                "1. **Triple-Channel Input**: [Log-Mel, Delta, Delta-Delta] capture static and dynamic sound patterns.\n",
                "2. **5-Fold Stratified CV**: Ensemble of 5 models trained from scratch.\n",
                "3. **Heavy Augmentations**: TimeShift, Mixup, and SpecAugment (Frequency/Time masking).\n",
                "4. **Optimization**: AdamW + OneCycleLR + Label Smoothing."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, json, random, numpy as np, pandas as pd, torch, torch.nn as nn, torch.nn.functional as F, torchaudio, torchvision.models as models\n",
                "from tqdm.auto import tqdm\n",
                "from torch.utils.data import DataLoader, Dataset\n",
                "from sklearn.model_selection import StratifiedKFold\n",
                "from sklearn.metrics import accuracy_score\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "def seed_everything(seed=42):\n",
                "    random.seed(seed); os.environ['PYTHONHASHSEED'] = str(seed); np.random.seed(seed)\n",
                "    torch.manual_seed(seed); torch.cuda.manual_seed(seed)\n",
                "    torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
                "\n",
                "seed_everything(42)\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f'Device: {device}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CFG:\n",
                "    data_dir = '/kaggle/input/the-last-frequency'\n",
                "    sample_rate, n_fft, hop_length, n_mels, target_frames = 16000, 1024, 256, 128, 64\n",
                "    n_splits, batch_size, epochs, lr, weight_decay, label_smoothing, mixup_alpha = 5, 64, 40, 1e-3, 1e-2, 0.1, 0.2\n",
                "    num_classes = 35"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Loading data...\")\n",
                "train_waveforms = np.load(f'{CFG.data_dir}/train_waveforms.npy')\n",
                "train_labels = np.load(f'{CFG.data_dir}/train_labels.npy')\n",
                "with open(f'{CFG.data_dir}/label_map.json') as f: \n",
                "    label_map = {int(k): v for k, v in json.load(f).items()}\n",
                "print(f'Data loaded: {train_waveforms.shape}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SpecTransform(nn.Module):\n",
                "    \"\"\"Advanced 3-channel feature extractor\"\"\"\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.mel_spec = torchaudio.transforms.MelSpectrogram(\n",
                "            sample_rate=CFG.sample_rate, n_fft=CFG.n_fft, \n",
                "            hop_length=CFG.hop_length, n_mels=CFG.n_mels\n",
                "        )\n",
                "        self.amp_to_db = torchaudio.transforms.AmplitudeToDB()\n",
                "        self.freq_mask = torchaudio.transforms.FrequencyMasking(25)\n",
                "        self.time_mask = torchaudio.transforms.TimeMasking(20)\n",
                "\n",
                "    def forward(self, x, augment=False):\n",
                "        # 1. Base Mel Spectrogram\n",
                "        spec = self.amp_to_db(self.mel_spec(x))\n",
                "        \n",
                "        # 2. Resizing/Padding\n",
                "        if spec.shape[-1] > CFG.target_frames: spec = spec[..., :CFG.target_frames]\n",
                "        elif spec.shape[-1] < CFG.target_frames: spec = F.pad(spec, (0, CFG.target_frames - spec.shape[-1]))\n",
                "            \n",
                "        if augment:\n",
                "            spec = self.freq_mask(spec)\n",
                "            spec = self.time_mask(spec)\n",
                "            \n",
                "        # 3. Compute Delta Features\n",
                "        # spec is (batch, n_mels, time)\n",
                "        delta = torchaudio.functional.compute_deltas(spec)\n",
                "        delta2 = torchaudio.functional.compute_deltas(delta)\n",
                "        \n",
                "        # 4. Stack into (batch, 3, n_mels, time)\n",
                "        return torch.stack([spec, delta, delta2], dim=1)\n",
                "\n",
                "class AudioResNet(nn.Module):\n",
                "    def __init__(self, num_classes=35):\n",
                "        super().__init__()\n",
                "        model = models.resnet18(weights=None)\n",
                "        # Original ResNet uses 3 channels, which we now provide with Deltas\n",
                "        model.fc = nn.Sequential(nn.Dropout(0.3), nn.Linear(model.fc.in_features, num_classes))\n",
                "        self.backbone = model\n",
                "        self.spec_layer = SpecTransform()\n",
                "\n",
                "    def forward(self, x, augment=False):\n",
                "        x = self.spec_layer(x, augment=augment)\n",
                "        return self.backbone(x)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SpeechDataset(Dataset):\n",
                "    def __init__(self, waveforms, labels=None, augment=False):\n",
                "        self.waveforms, self.labels, self.augment = waveforms, labels, augment\n",
                "    def __len__(self): return len(self.waveforms)\n",
                "    def __getitem__(self, idx):\n",
                "        wav = self.waveforms[idx].copy()\n",
                "        if self.augment: wav = np.roll(wav, int(random.uniform(-0.1, 0.1) * wav.shape[0]))\n",
                "        wav = torch.from_numpy(wav).float()\n",
                "        return (wav, self.labels[idx]) if self.labels is not None else wav\n",
                "\n",
                "def mixup_data(x, y, alpha=0.2):\n",
                "    lam = np.random.beta(alpha, alpha) if alpha > 0 else 1\n",
                "    index = torch.randperm(x.size()[0]).to(device)\n",
                "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
                "    return mixed_x, y, y[index], lam"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=42)\n",
                "for fold, (train_idx, val_idx) in enumerate(skf.split(train_waveforms, train_labels)):\n",
                "    print(f\"\\nFold {fold+1}/{CFG.n_splits}\")\n",
                "    train_loader = DataLoader(SpeechDataset(train_waveforms[train_idx], train_labels[train_idx], augment=True), batch_size=CFG.batch_size, shuffle=True, num_workers=2)\n",
                "    val_loader = DataLoader(SpeechDataset(train_waveforms[val_idx], train_labels[val_idx], augment=False), batch_size=CFG.batch_size, shuffle=False)\n",
                "    \n",
                "    model = AudioResNet(CFG.num_classes).to(device)\n",
                "    optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
                "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=CFG.lr*2, steps_per_epoch=len(train_loader), epochs=CFG.epochs)\n",
                "    criterion = nn.CrossEntropyLoss(label_smoothing=CFG.label_smoothing)\n",
                "    \n",
                "    best_acc = 0\n",
                "    for epoch in range(1, CFG.epochs + 1):\n",
                "        model.train()\n",
                "        for x, y in train_loader:\n",
                "            x, y = x.to(device), y.to(device)\n",
                "            if random.random() < 0.5:\n",
                "                x, y_a, y_b, lam = mixup_data(x, y, CFG.mixup_alpha)\n",
                "                preds = model(x, augment=True)\n",
                "                loss = lam * criterion(preds, y_a) + (1 - lam) * criterion(preds, y_b)\n",
                "            else:\n",
                "                preds = model(x, augment=True); loss = criterion(preds, y)\n",
                "            optimizer.zero_grad(); loss.backward(); optimizer.step(); scheduler.step()\n",
                "            \n",
                "        model.eval(); vp, vt = [], []\n",
                "        with torch.no_grad():\n",
                "            for x, y in val_loader:\n",
                "                out = model(x.to(device))\n",
                "                vp.extend(out.argmax(1).cpu().numpy()); vt.extend(y.numpy())\n",
                "        \n",
                "        acc = accuracy_score(vt, vp)\n",
                "        if acc > best_acc:\n",
                "            best_acc = acc; torch.save(model.state_dict(), f'best_model_fold_{fold}.pth')\n",
                "        if epoch % 10 == 0: print(f\"Epoch {epoch} Val Acc: {acc:.4f} (Best: {best_acc:.4f})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_fold_probs(waveforms):\n",
                "    all_probs = []\n",
                "    loader = DataLoader(SpeechDataset(waveforms, augment=False), batch_size=CFG.batch_size, shuffle=False)\n",
                "    for fold in range(CFG.n_splits):\n",
                "        m = AudioResNet(CFG.num_classes).to(device)\n",
                "        m.load_state_dict(torch.load(f'best_model_fold_{fold}.pth'))\n",
                "        m.eval(); fold_prob = []\n",
                "        with torch.no_grad():\n",
                "            for x in tqdm(loader): \n",
                "                probs = F.softmax(m(x.to(device)), dim=1)\n",
                "                fold_prob.append(probs.cpu().numpy())\n",
                "        all_probs.append(np.concatenate(fold_prob))\n",
                "    return np.mean(all_probs, axis=0)\n",
                "\n",
                "pub, priv = np.load(f'{CFG.data_dir}/public_test_waveforms.npy'), np.load(f'{CFG.data_dir}/private_test_waveforms.npy')\n",
                "final_probs = np.concatenate([get_fold_probs(pub), get_fold_probs(priv)])\n",
                "final_cmds = [label_map[idx] for idx in final_probs.argmax(1)]\n",
                "pd.DataFrame({'Id': range(len(final_cmds)), 'Command': final_cmds}).to_csv('submission.csv', index=False)\n",
                "print(\"Final submission saved with Delta features!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}