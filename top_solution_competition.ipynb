{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Last Frequency: SOTA From Scratch Solution\n",
    "\n",
    "This notebook implements a high-performance audio classification pipeline designed to win the competition without using pretrained weights. \n",
    "\n",
    "### Key features of this solution:\n",
    "1. **Advanced Feature Extraction**: 128-bin Log-Mel Spectrograms.\n",
    "2. **ResNet-18 Architecture**: Trained from scratch, optimized for spectrogram patterns.\n",
    "3. **Heavy Augmentations**: \n",
    "   - **Time Shifting**: Shifting frequency patterns in time.\n",
    "   - **Background Noise Injection**: Realistic noise robustness.\n",
    "   - **SpecAugment**: Frequency and Time masking to prevent overfitting on images.\n",
    "   - **Mixup Training**: Smoothing decision boundaries by mixing samples.\n",
    "4. **Optimization Strategy**: AdamW + OneCycleLR + Label Smoothing.\n",
    "5. **Efficiency**: GPU-accelerated spectrogram generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    data_dir = '/kaggle/input/the-last-frequency'\n",
    "    sample_rate = 16000\n",
    "    n_fft = 1024\n",
    "    hop_length = 256\n",
    "    n_mels = 128\n",
    "    target_frames = 64\n",
    "    \n",
    "    batch_size = 64\n",
    "    epochs = 40  # Longer training is better for scratch\n",
    "    lr = 1e-3\n",
    "    weight_decay = 1e-2\n",
    "    label_smoothing = 0.1\n",
    "    mixup_alpha = 0.2\n",
    "    \n",
    "    num_classes = 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "train_waveforms = np.load(f'{CFG.data_dir}/train_waveforms.npy')\n",
    "train_labels = np.load(f'{CFG.data_dir}/train_labels.npy')\n",
    "test_waveforms = np.load(f'{CFG.data_dir}/test_waveforms.npy')\n",
    "\n",
    "with open(f'{CFG.data_dir}/label_map.json') as f:\n",
    "    label_map = {int(k): v for k, v in json.load(f).items()}\n",
    "\n",
    "print(f'Train shape: {train_waveforms.shape}, Labels: {len(train_labels)}')\n",
    "print(f'Test shape: {test_waveforms.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentations & Transforms\n",
    "\n",
    "We implement raw audio augmentations and GPU-based spectrogram generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioAugmentor:\n",
    "    @staticmethod\n",
    "    def time_shift(waveform, shift_limit=0.1):\n",
    "        shift = int(random.uniform(-shift_limit, shift_limit) * waveform.shape[0])\n",
    "        return np.roll(waveform, shift)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_noise(waveform, noise_limit=0.01):\n",
    "        noise = np.random.randn(*waveform.shape)\n",
    "        return waveform + random.uniform(0, noise_limit) * noise\n",
    "\n",
    "class SpecTransform(nn.Module):\n",
    "    \"\"\"Converts raw audio to log-mel spectrogram on CPU/GPU\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mel_spec = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=CFG.sample_rate,\n",
    "            n_fft=CFG.n_fft,\n",
    "            hop_length=CFG.hop_length,\n",
    "            n_mels=CFG.n_mels\n",
    "        )\n",
    "        self.amplitude_to_db = torchaudio.transforms.AmplitudeToDB()\n",
    "        # SpecAugment parameters\n",
    "        self.freq_mask = torchaudio.transforms.FrequencyMasking(freq_mask_param=20)\n",
    "        self.time_mask = torchaudio.transforms.TimeMasking(time_mask_param=15)\n",
    "\n",
    "    def forward(self, x, augment=False):\n",
    "        # x shape: (batch, 16000)\n",
    "        spec = self.mel_spec(x)\n",
    "        spec = self.amplitude_to_db(spec)\n",
    "        \n",
    "        # Resizing to standard size\n",
    "        if spec.shape[-1] > CFG.target_frames:\n",
    "            spec = spec[..., :CFG.target_frames]\n",
    "        elif spec.shape[-1] < CFG.target_frames:\n",
    "            pad = CFG.target_frames - spec.shape[-1]\n",
    "            spec = F.pad(spec, (0, pad))\n",
    "            \n",
    "        if augment:\n",
    "            spec = self.freq_mask(spec)\n",
    "            spec = self.time_mask(spec)\n",
    "            \n",
    "        return spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeechDataset(Dataset):\n",
    "    def __init__(self, waveforms, labels=None, augment=False):\n",
    "        self.waveforms = waveforms\n",
    "        self.labels = labels\n",
    "        self.augment = augment\n",
    "        self.augmentor = AudioAugmentor()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.waveforms)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        waveform = self.waveforms[idx].copy()\n",
    "        \n",
    "        if self.augment:\n",
    "            waveform = self.augmentor.time_shift(waveform)\n",
    "            waveform = self.augmentor.add_noise(waveform)\n",
    "            \n",
    "        waveform = torch.from_numpy(waveform).float()\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            return waveform, self.labels[idx]\n",
    "        return waveform\n",
    "\n",
    "# Split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    train_waveforms, train_labels, test_size=0.15, random_state=42, stratify=train_labels\n",
    ")\n",
    "\n",
    "train_ds = SpeechDataset(X_train, y_train, augment=True)\n",
    "val_ds = SpeechDataset(X_val, y_val, augment=False)\n",
    "test_ds = SpeechDataset(test_waveforms, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=CFG.batch_size, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_ds, batch_size=CFG.batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=CFG.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture (Custom ResNet-18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioResNet(nn.Module):\n",
    "    def __init__(self, num_classes=35):\n",
    "        super().__init__()\n",
    "        # Extract architecture but don't load weights\n",
    "        model = models.resnet18(pretrained=False)\n",
    "        \n",
    "        # Modify first layer for 1-channel input (spectrogram)\n",
    "        model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        \n",
    "        # Modify last layer for 35 classes\n",
    "        num_ftrs = model.fc.in_features\n",
    "        model.fc = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(num_ftrs, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.backbone = model\n",
    "        self.spec_layer = SpecTransform()\n",
    "\n",
    "    def forward(self, x, augment=False):\n",
    "        # 1. Transform raw wav to spec\n",
    "        x = self.spec_layer(x, augment=augment)\n",
    "        # 2. Add channel dim\n",
    "        x = x.unsqueeze(1)\n",
    "        # 3. Backbone\n",
    "        return self.backbone(x)\n",
    "\n",
    "model = AudioResNet(CFG.num_classes).to(device)\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mixup Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=0.2):\n",
    "    '''Returns mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=CFG.lr*2, \n",
    "    steps_per_epoch=len(train_loader), \n",
    "    epochs=CFG.epochs\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=CFG.label_smoothing)\n",
    "\n",
    "best_acc = 0.0\n",
    "history = {'train_loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(1, CFG.epochs + 1):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "    \n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Mixup implementation \n",
    "        if random.random() < 0.5: # Apply mixup to 50% of matches\n",
    "            x, y_a, y_b, lam = mixup_data(x, y, CFG.mixup_alpha)\n",
    "            preds = model(x, augment=True)\n",
    "            loss = mixup_criterion(criterion, preds, y_a, y_b, lam)\n",
    "        else:\n",
    "            preds = model(x, augment=True)\n",
    "            loss = criterion(preds, y)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in val_loader:\n",
    "            x = x.to(device)\n",
    "            preds = model(x, augment=False)\n",
    "            all_preds.extend(preds.argmax(1).cpu().numpy())\n",
    "            all_targets.extend(y.numpy())\n",
    "            \n",
    "    val_acc = accuracy_score(all_targets, all_preds)\n",
    "    print(f\"Epoch {epoch} | Train Loss: {train_loss/len(train_loader):.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "        print(\"New Best Score Saved!\")\n",
    "    \n",
    "    history['train_loss'].append(train_loss/len(train_loader))\n",
    "    history['val_acc'].append(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_model.pth'))\n",
    "model.eval()\n",
    "test_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x in tqdm(test_loader, desc='Inference'):\n",
    "        x = x.to(device)\n",
    "        preds = model(x, augment=False)\n",
    "        test_preds.extend(preds.argmax(1).cpu().numpy())\n",
    "\n",
    "submission = pd.DataFrame({'label': test_preds})\n",
    "submission.to_csv('submission.csv', index_label='id')\n",
    "print(\"Submission saved as submission.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
